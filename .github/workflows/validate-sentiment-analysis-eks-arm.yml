name: Validate Learning Path Commands (Arm) - Sentiment Analysis EKS

on:
  push:
    paths:
      - 'content/learning-paths/servers-and-cloud-computing/sentiment-analysis-eks/**'
  pull_request:
    paths:
      - 'content/learning-paths/servers-and-cloud-computing/sentiment-analysis-eks/**'
  workflow_dispatch:

env:
  DEBIAN_FRONTEND: noninteractive

jobs:
  validate:
    runs-on: ubuntu-22.04-arm64
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      # ========================================================================
      # Setup and Dependencies
      # ========================================================================
      
      - name: Update package lists
        run: |
          sudo apt-get update

      - name: Install Docker Compose plugin
        run: |
          sudo apt-get install -y docker-compose-plugin

      - name: Install sbt (Scala Build Tool)
        run: |
          echo "deb https://repo.scala-sbt.org/scalasbt/debian all main" | sudo tee /etc/apt/sources.list.d/sbt.list
          curl -fsSL "https://keyserver.ubuntu.com/pks/lookup?op=get&search=0x2EE0EA64E40A89B84B2DF73499E82A75642AC823" | sudo gpg --dearmor -o /usr/share/keyrings/sbt-archive-keyring.gpg
          sudo apt-get update
          sudo apt-get install -y sbt

      - name: Verify sbt installation
        run: |
          sbt --version

      # ========================================================================
      # Elasticsearch and Kibana Setup
      # ========================================================================

      - name: Create docker-compose.yml for Elasticsearch and Kibana
        run: |
          cat > docker-compose.yml << 'EOF'
          services:
            elasticsearch:
              image: elasticsearch:8.16.1
              container_name: elasticsearch
              environment:
                - discovery.type=single-node
                - ES_JAVA_OPTS=-Xms512m -Xmx512m
                - xpack.security.enabled=false
                - HTTP_ENABLE=true
              ports:
                - "9200:9200"
              networks:
                - elk

            kibana:
              image: kibana:8.16.1
              container_name: kibana
              ports:
                - "5601:5601"
              environment:
                - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
                - HTTP_ENABLE=true
              depends_on:
                - elasticsearch
              networks:
                - elk

          networks:
            elk:
              driver: bridge
          EOF

      - name: Start Elasticsearch and Kibana (background)
        run: |
          docker compose up -d

      - name: Wait for Elasticsearch to be ready
        run: |
          echo "Waiting for Elasticsearch to start..."
          for i in {1..30}; do
            if curl -s http://localhost:9200 > /dev/null; then
              echo "Elasticsearch is ready"
              break
            fi
            echo "Attempt $i: Elasticsearch not ready yet, waiting..."
            sleep 5
          done
          curl -s http://localhost:9200 || echo "Elasticsearch failed to start"

      - name: Wait for Kibana to be ready
        run: |
          echo "Waiting for Kibana to start..."
          for i in {1..30}; do
            if curl -s http://localhost:5601/api/status > /dev/null; then
              echo "Kibana is ready"
              break
            fi
            echo "Attempt $i: Kibana not ready yet, waiting..."
            sleep 5
          done
          curl -s http://localhost:5601/api/status || echo "Kibana failed to start"

      # ========================================================================
      # Repository Clone and Build
      # ========================================================================

      - name: Clone spark-sentiment-analysis repository
        run: |
          git clone https://github.com/koleini/spark-sentiment-analysis.git
          cd spark-sentiment-analysis/eks
          ls -la

      - name: Verify repository structure
        run: |
          cd spark-sentiment-analysis
          ls -la
          test -d sentiment_analysis || echo "WARNING: sentiment_analysis directory not found"
          test -d eks || echo "WARNING: eks directory not found"

      # TODO: Terraform commands require AWS credentials and will create actual infrastructure
      # These commands are commented out for CI safety
      # - name: Initialize Terraform
      #   run: |
      #     cd spark-sentiment-analysis/eks
      #     terraform init

      # - name: Apply Terraform configuration
      #   run: |
      #     cd spark-sentiment-analysis/eks
      #     terraform apply --auto-approve

      # ========================================================================
      # Build JAR File
      # ========================================================================

      - name: Build sentiment analysis JAR file
        run: |
          cd spark-sentiment-analysis/sentiment_analysis
          sbt assembly
        timeout-minutes: 15

      - name: Verify JAR file creation
        run: |
          cd spark-sentiment-analysis
          test -f sentiment_analysis/target/scala-2.13/bigdata-assembly-0.1.jar && \
            echo "JAR file created successfully" || \
            echo "ERROR: JAR file not found"
          ls -lh sentiment_analysis/target/scala-2.13/ || true

      # ========================================================================
      # Apache Spark Setup
      # ========================================================================

      - name: Download Apache Spark
        run: |
          cd spark-sentiment-analysis
          wget https://archive.apache.org/dist/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3-scala2.13.tgz
          tar -xzf spark-3.5.3-bin-hadoop3-scala2.13.tgz

      - name: Verify Spark extraction
        run: |
          cd spark-sentiment-analysis
          test -d spark-3.5.3-bin-hadoop3-scala2.13 && \
            echo "Spark extracted successfully" || \
            echo "ERROR: Spark directory not found"
          ls -la spark-3.5.3-bin-hadoop3-scala2.13/bin/ || true

      - name: Copy JAR file to Spark jars directory
        run: |
          cd spark-sentiment-analysis
          cp sentiment_analysis/target/scala-2.13/bigdata-assembly-0.1.jar \
             spark-3.5.3-bin-hadoop3-scala2.13/jars/
          ls -lh spark-3.5.3-bin-hadoop3-scala2.13/jars/bigdata-assembly-0.1.jar

      # TODO: Docker image build requires container registry credentials
      # - name: Build Spark Docker image
      #   run: |
      #     cd spark-sentiment-analysis/spark-3.5.3-bin-hadoop3-scala2.13
      #     bin/docker-image-tool.sh -r <your-docker-repository> -t sentiment-analysis build

      # TODO: Docker push requires authentication
      # - name: Push Spark Docker image
      #   run: |
      #     cd spark-sentiment-analysis/spark-3.5.3-bin-hadoop3-scala2.13
      #     bin/docker-image-tool.sh -r <your-docker-repository> -t sentiment-analysis push

      # ========================================================================
      # Kubernetes Commands (require active cluster)
      # ========================================================================

      # TODO: kubectl commands require active EKS cluster
      # - name: Update kubeconfig
      #   run: |
      #     aws eks --region $(terraform output -raw region) update-kubeconfig \
      #       --name $(terraform output -raw cluster_name) --profile <Your_AWS_Profile>

      # TODO: Requires active Kubernetes cluster
      # - name: Create Spark service account
      #   run: |
      #     kubectl create serviceaccount spark
      #     kubectl create clusterrolebinding spark-role --clusterrole=edit \
      #       --serviceaccount=default:spark --namespace=default

      # TODO: spark-submit requires active EKS cluster and ECR image
      # - name: Submit Spark job to cluster
      #   run: |
      #     cd spark-sentiment-analysis/spark-3.5.3-bin-hadoop3-scala2.13
      #     export K8S_API_SERVER_ADDRESS=<endpoint>
      #     export ES_ADDRESS=localhost
      #     export CHECKPOINT_BUCKET=<bucket>
      #     export ECR_ADDRESS=<ecr>
      #     bin/spark-submit --class bigdata.SentimentAnalysis ...

      # ========================================================================
      # Helm and Monitoring (require active cluster)
      # ========================================================================

      # TODO: Helm commands require active Kubernetes cluster
      # - name: Verify Helm installation
      #   run: |
      #     helm version

      # - name: Create Prometheus namespace
      #   run: |
      #     kubectl create namespace prometheus

      # - name: Add Prometheus Helm repo
      #   run: |
      #     helm repo add prometheus-community https://prometheus-community.github.io/helm-charts

      # - name: Install Prometheus
      #   run: |
      #     helm install prometheus prometheus-community/prometheus \
      #       --namespace prometheus \
      #       --set alertmanager.persistentVolume.storageClass="gp2" \
      #       --set server.persistentVolume.storageClass="gp2"

      # - name: Create Grafana namespace
      #   run: |
      #     kubectl create namespace grafana

      # - name: Add Grafana Helm repo
      #   run: |
      #     helm repo add grafana https://grafana.github.io/helm-charts

      # - name: Create Grafana configuration
      #   run: |
      #     cat > grafana.yaml << 'EOF'
      #     datasources:
      #       datasources.yaml:
      #         apiVersion: 1
      #         datasources:
      #         - name: Prometheus
      #           type: prometheus
      #           url: http://prometheus-server.prometheus.svc.cluster.local
      #           access: proxy
      #           isDefault: true
      #     EOF

      # - name: Install Grafana
      #   run: |
      #     helm install grafana grafana/grafana \
      #       --namespace grafana \
      #       --set persistence.storageClassName="gp2" \
      #       --set persistence.enabled=true \
      #       --set adminPassword='kubegrafana' \
      #       --values grafana.yaml \
      #       --set service.type=LoadBalancer

      # ========================================================================
      # Python Dependencies (X API integration)
      # ========================================================================

      - name: Install Python dependencies
        run: |
          pip3 install requests boto3

      - name: Verify Python packages
        run: |
          python3 -c "import requests; print('requests:', requests.__version__)"
          python3 -c "import boto3; print('boto3:', boto3.__version__)"

      # TODO: X API commands require bearer token and active cluster
      # - name: Run X API tweets script
      #   run: |
      #     cd spark-sentiment-analysis
      #     export BEARER_TOKEN=<token>
      #     python3 scripts/xapi_tweets.py

      # - name: Send data to Elasticsearch
      #   run: |
      #     cd spark-sentiment-analysis
      #     python3 csv_to_kinesis.py

      # ========================================================================
      # Cleanup
      # ========================================================================

      - name: Stop Elasticsearch and Kibana
        if: always()
        run: |
          docker compose down || true

      # TODO: Terraform destroy requires active infrastructure
      # - name: Cleanup Terraform resources
      #   if: always()
      #   run: |
      #     cd spark-sentiment-analysis/eks
      #     terraform destroy --auto-approve

      # ========================================================================
      # Summary
      # ========================================================================

      - name: Validation summary
        if: always()
        run: |
          echo "=========================================="
          echo "Validation Summary"
          echo "=========================================="
          echo "✅ sbt installed and verified"
          echo "✅ Docker Compose installed"
          echo "✅ Elasticsearch and Kibana containers started"
          echo "✅ Repository cloned successfully"
          echo "✅ JAR file built with sbt assembly"
          echo "✅ Apache Spark downloaded and extracted"
          echo "✅ Python dependencies installed"
          echo ""
          echo "⚠️  Skipped: Terraform (requires AWS credentials)"
          echo "⚠️  Skipped: kubectl/EKS (requires active cluster)"
          echo "⚠️  Skipped: Docker image build/push (requires registry)"
          echo "⚠️  Skipped: Helm/Prometheus/Grafana (requires cluster)"
          echo "⚠️  Skipped: X API integration (requires bearer token)"
          echo "=========================================="
